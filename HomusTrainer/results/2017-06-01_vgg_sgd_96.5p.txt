C:\Programming\Anaconda3-4.2.0\python.exe C:/Users/Alex/Repositories/MusicSymbolClassifier/HomusTrainer/TrainModel.py
Using TensorFlow backend.
Deleting dataset directory data
Extracting HOMUS Dataset...
Generating 15200 images with 15200 symbols in 1 different stroke thicknesses
15200/15200Deleting split directories... 
Splitting data into training, validation and test sets...
Copying 320 training files of 12-8-Time...
Copying 40 validation files of 12-8-Time...
Copying 40 test files of 12-8-Time...
Copying 318 training files of 2-2-Time...
Copying 39 validation files of 2-2-Time...
Copying 39 test files of 2-2-Time...
Copying 320 training files of 2-4-Time...
Copying 40 validation files of 2-4-Time...
Copying 40 test files of 2-4-Time...
Copying 320 training files of 3-4-Time...
Copying 40 validation files of 3-4-Time...
Copying 40 test files of 3-4-Time...
Copying 320 training files of 3-8-Time...
Copying 40 validation files of 3-8-Time...
Copying 40 test files of 3-8-Time...
Copying 320 training files of 4-4-Time...
Copying 40 validation files of 4-4-Time...
Copying 40 test files of 4-4-Time...
Copying 320 training files of 6-8-Time...
Copying 40 validation files of 6-8-Time...
Copying 40 test files of 6-8-Time...
Copying 320 training files of 9-8-Time...
Copying 40 validation files of 9-8-Time...
Copying 40 test files of 9-8-Time...
Copying 322 training files of Barline...
Copying 40 validation files of Barline...
Copying 40 test files of Barline...
Copying 320 training files of C-Clef...
Copying 40 validation files of C-Clef...
Copying 40 test files of C-Clef...
Copying 320 training files of Common-Time...
Copying 40 validation files of Common-Time...
Copying 40 test files of Common-Time...
Copying 324 training files of Cut-Time...
Copying 40 validation files of Cut-Time...
Copying 40 test files of Cut-Time...
Copying 320 training files of Dot...
Copying 40 validation files of Dot...
Copying 40 test files of Dot...
Copying 320 training files of Double-Sharp...
Copying 40 validation files of Double-Sharp...
Copying 40 test files of Double-Sharp...
Copying 640 training files of Eighth-Note...
Copying 80 validation files of Eighth-Note...
Copying 80 test files of Eighth-Note...
Copying 320 training files of Eighth-Rest...
Copying 40 validation files of Eighth-Rest...
Copying 40 test files of Eighth-Rest...
Copying 320 training files of F-Clef...
Copying 40 validation files of F-Clef...
Copying 40 test files of F-Clef...
Copying 321 training files of Flat...
Copying 39 validation files of Flat...
Copying 39 test files of Flat...
Copying 320 training files of G-Clef...
Copying 40 validation files of G-Clef...
Copying 40 test files of G-Clef...
Copying 641 training files of Half-Note...
Copying 79 validation files of Half-Note...
Copying 79 test files of Half-Note...
Copying 320 training files of Natural...
Copying 40 validation files of Natural...
Copying 40 test files of Natural...
Copying 641 training files of Quarter-Note...
Copying 80 validation files of Quarter-Note...
Copying 80 test files of Quarter-Note...
Copying 320 training files of Quarter-Rest...
Copying 40 validation files of Quarter-Rest...
Copying 40 test files of Quarter-Rest...
Copying 320 training files of Sharp...
Copying 40 validation files of Sharp...
Copying 40 test files of Sharp...
Copying 641 training files of Sixteenth-Note...
Copying 80 validation files of Sixteenth-Note...
Copying 80 test files of Sixteenth-Note...
Copying 320 training files of Sixteenth-Rest...
Copying 40 validation files of Sixteenth-Rest...
Copying 40 test files of Sixteenth-Rest...
Copying 641 training files of Sixty-Four-Note...
Copying 79 validation files of Sixty-Four-Note...
Copying 79 test files of Sixty-Four-Note...
Copying 320 training files of Sixty-Four-Rest...
Copying 40 validation files of Sixty-Four-Rest...
Copying 40 test files of Sixty-Four-Rest...
Copying 641 training files of Thirty-Two-Note...
Copying 79 validation files of Thirty-Two-Note...
Copying 79 test files of Thirty-Two-Note...
Copying 320 training files of Thirty-Two-Rest...
Copying 40 validation files of Thirty-Two-Rest...
Copying 40 test files of Thirty-Two-Rest...
Copying 320 training files of Whole-Half-Rest...
Copying 40 validation files of Whole-Half-Rest...
Copying 40 test files of Whole-Half-Rest...
Copying 320 training files of Whole-Note...
Copying 40 validation files of Whole-Note...
Copying 40 test files of Whole-Note...
Training on dataset...
Found 12170 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
Found 1515 images belonging to 32 classes.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 224, 128, 16)      448       
_________________________________________________________________
batch_normalization_1 (Batch (None, 224, 128, 16)      64        
_________________________________________________________________
activation_1 (Activation)    (None, 224, 128, 16)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 224, 128, 16)      2320      
_________________________________________________________________
batch_normalization_2 (Batch (None, 224, 128, 16)      64        
_________________________________________________________________
activation_2 (Activation)    (None, 224, 128, 16)      0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 112, 64, 16)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 112, 64, 32)       4640      
_________________________________________________________________
batch_normalization_3 (Batch (None, 112, 64, 32)       128       
_________________________________________________________________
activation_3 (Activation)    (None, 112, 64, 32)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 112, 64, 32)       9248      
_________________________________________________________________
batch_normalization_4 (Batch (None, 112, 64, 32)       128       
_________________________________________________________________
activation_4 (Activation)    (None, 112, 64, 32)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 56, 32, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 56, 32, 64)        18496     
_________________________________________________________________
batch_normalization_5 (Batch (None, 56, 32, 64)        256       
_________________________________________________________________
activation_5 (Activation)    (None, 56, 32, 64)        0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 56, 32, 64)        36928     
_________________________________________________________________
batch_normalization_6 (Batch (None, 56, 32, 64)        256       
_________________________________________________________________
activation_6 (Activation)    (None, 56, 32, 64)        0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 56, 32, 64)        36928     
_________________________________________________________________
batch_normalization_7 (Batch (None, 56, 32, 64)        256       
_________________________________________________________________
activation_7 (Activation)    (None, 56, 32, 64)        0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 28, 16, 64)        0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 28, 16, 128)       73856     
_________________________________________________________________
batch_normalization_8 (Batch (None, 28, 16, 128)       512       
_________________________________________________________________
activation_8 (Activation)    (None, 28, 16, 128)       0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 28, 16, 128)       147584    
_________________________________________________________________
batch_normalization_9 (Batch (None, 28, 16, 128)       512       
_________________________________________________________________
activation_9 (Activation)    (None, 28, 16, 128)       0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 28, 16, 128)       147584    
_________________________________________________________________
batch_normalization_10 (Batc (None, 28, 16, 128)       512       
_________________________________________________________________
activation_10 (Activation)   (None, 28, 16, 128)       0         
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 14, 8, 128)        0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 14, 8, 192)        221376    
_________________________________________________________________
batch_normalization_11 (Batc (None, 14, 8, 192)        768       
_________________________________________________________________
activation_11 (Activation)   (None, 14, 8, 192)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 14, 8, 192)        331968    
_________________________________________________________________
batch_normalization_12 (Batc (None, 14, 8, 192)        768       
_________________________________________________________________
activation_12 (Activation)   (None, 14, 8, 192)        0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 14, 8, 192)        331968    
_________________________________________________________________
batch_normalization_13 (Batc (None, 14, 8, 192)        768       
_________________________________________________________________
activation_13 (Activation)   (None, 14, 8, 192)        0         
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 14, 8, 192)        331968    
_________________________________________________________________
batch_normalization_14 (Batc (None, 14, 8, 192)        768       
_________________________________________________________________
activation_14 (Activation)   (None, 14, 8, 192)        0         
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 7, 4, 192)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 5376)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                172064    
_________________________________________________________________
output_node (Activation)     (None, 32)                0         
=================================================================
Total params: 1,873,136
Trainable params: 1,870,256
Non-trainable params: 2,880
_________________________________________________________________
Model vgg loaded.
2017-05-31 21:22:01.452185: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-05-31 21:22:01.452438: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-31 21:22:01.452672: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-31 21:22:01.452895: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-31 21:22:01.453120: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-31 21:22:01.453342: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-05-31 21:22:01.453568: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-31 21:22:01.453803: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-05-31 21:22:01.775148: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:887] Found device 0 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:01:00.0
Total memory: 11.00GiB
Free memory: 9.12GiB
2017-05-31 21:22:01.775406: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:908] DMA: 0 
2017-05-31 21:22:01.775565: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:918] 0:   Y 
2017-05-31 21:22:01.775710: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)
Training for 200 epochs with initial learning rate of 0.01, weight-decay of 0.0001 and Nesterov Momentum of 0.9 ...
Additional parameters: Initialization: glorot_uniform, Minibatch-size: 64, Early stopping after 20 epochs without improvement
Data-Shape: (224, 128, 3), Reducing learning rate by factor to 0.5 respectively if not improved validation accuracy after 8 epochs
Data-augmentation: Zooming 20.0% randomly, rotating 10Â° randomly
Optimizer: SGD, with parameters {'momentum': 0.8999999761581421, 'nesterov': True, 'lr': 0.009999999776482582, 'decay': 0.0}
Epoch 1/200
  9/191 [>.............................] - ETA: 115s - loss: 8.9914 - acc: 0.03992017-05-31 21:22:14.793250: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:247] PoolAllocator: After 3612 get requests, put_count=3523 evicted_count=1000 eviction_rate=0.283849 and unsatisfied allocation rate=0.329181
2017-05-31 21:22:14.793971: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
190/191 [============================>.] - ETA: 0s - loss: 5.8770 - acc: 0.0656Epoch 00000: val_acc improved from -inf to 0.02640, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 69s - loss: 5.8849 - acc: 0.0658 - val_loss: 15.8530 - val_acc: 0.0264
Epoch 2/200
190/191 [============================>.] - ETA: 0s - loss: 3.5796 - acc: 0.1422Epoch 00001: val_acc improved from 0.02640 to 0.26469, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 61s - loss: 3.5798 - acc: 0.1441 - val_loss: 2.6623 - val_acc: 0.2647
Epoch 3/200
190/191 [============================>.] - ETA: 0s - loss: 2.4510 - acc: 0.3384Epoch 00002: val_acc did not improve
191/191 [==============================] - 61s - loss: 2.4434 - acc: 0.3403 - val_loss: 5.2786 - val_acc: 0.1472
Epoch 4/200
190/191 [============================>.] - ETA: 0s - loss: 1.5748 - acc: 0.5355Epoch 00003: val_acc did not improve
191/191 [==============================] - 61s - loss: 1.5732 - acc: 0.5359 - val_loss: 2.9642 - val_acc: 0.2429
Epoch 5/200
190/191 [============================>.] - ETA: 0s - loss: 1.0215 - acc: 0.7034Epoch 00004: val_acc improved from 0.26469 to 0.57492, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 61s - loss: 1.0240 - acc: 0.7028 - val_loss: 1.5060 - val_acc: 0.5749
Epoch 6/200
190/191 [============================>.] - ETA: 0s - loss: 0.7925 - acc: 0.7768Epoch 00005: val_acc improved from 0.57492 to 0.71221, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 61s - loss: 0.7912 - acc: 0.7780 - val_loss: 1.0788 - val_acc: 0.7122
Epoch 7/200
190/191 [============================>.] - ETA: 0s - loss: 0.6461 - acc: 0.8284Epoch 00006: val_acc improved from 0.71221 to 0.76370, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 61s - loss: 0.6504 - acc: 0.8272 - val_loss: 0.8046 - val_acc: 0.7637
Epoch 8/200
190/191 [============================>.] - ETA: 0s - loss: 0.5725 - acc: 0.8585Epoch 00007: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.5711 - acc: 0.8592 - val_loss: 2.2226 - val_acc: 0.5175
Epoch 9/200
190/191 [============================>.] - ETA: 0s - loss: 0.5097 - acc: 0.8785Epoch 00008: val_acc improved from 0.76370 to 0.77756, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 62s - loss: 0.5100 - acc: 0.8780 - val_loss: 0.9539 - val_acc: 0.7776
Epoch 10/200
190/191 [============================>.] - ETA: 0s - loss: 0.4785 - acc: 0.8936Epoch 00009: val_acc improved from 0.77756 to 0.78350, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 62s - loss: 0.4785 - acc: 0.8931 - val_loss: 0.8506 - val_acc: 0.7835
Epoch 11/200
190/191 [============================>.] - ETA: 0s - loss: 0.4449 - acc: 0.9022Epoch 00010: val_acc improved from 0.78350 to 0.83630, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 62s - loss: 0.4438 - acc: 0.9027 - val_loss: 0.6176 - val_acc: 0.8363
Epoch 12/200
190/191 [============================>.] - ETA: 0s - loss: 0.4248 - acc: 0.9091Epoch 00011: val_acc improved from 0.83630 to 0.89703, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 62s - loss: 0.4250 - acc: 0.9086 - val_loss: 0.4753 - val_acc: 0.8970
Epoch 13/200
190/191 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.9160Epoch 00012: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.4136 - acc: 0.9138 - val_loss: 1.5269 - val_acc: 0.6818
Epoch 14/200
190/191 [============================>.] - ETA: 0s - loss: 0.4010 - acc: 0.9146Epoch 00013: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.4018 - acc: 0.9140 - val_loss: 1.1222 - val_acc: 0.7591
Epoch 15/200
190/191 [============================>.] - ETA: 0s - loss: 0.3775 - acc: 0.9252Epoch 00014: val_acc improved from 0.89703 to 0.91287, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 62s - loss: 0.3770 - acc: 0.9256 - val_loss: 0.4330 - val_acc: 0.9129
Epoch 16/200
190/191 [============================>.] - ETA: 0s - loss: 0.3590 - acc: 0.9346Epoch 00015: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.3597 - acc: 0.9339 - val_loss: 0.5282 - val_acc: 0.8627
Epoch 17/200
190/191 [============================>.] - ETA: 0s - loss: 0.3455 - acc: 0.9363Epoch 00016: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.3466 - acc: 0.9361 - val_loss: 0.7295 - val_acc: 0.8535
Epoch 18/200
190/191 [============================>.] - ETA: 0s - loss: 0.3358 - acc: 0.9390Epoch 00017: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.3359 - acc: 0.9388 - val_loss: 0.5132 - val_acc: 0.8917
Epoch 19/200
190/191 [============================>.] - ETA: 0s - loss: 0.3207 - acc: 0.9460Epoch 00018: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.3248 - acc: 0.9442 - val_loss: 1.0198 - val_acc: 0.8026
Epoch 20/200
190/191 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.9372Epoch 00019: val_acc improved from 0.91287 to 0.93201, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 62s - loss: 0.3527 - acc: 0.9375 - val_loss: 0.3617 - val_acc: 0.9320
Epoch 21/200
190/191 [============================>.] - ETA: 0s - loss: 0.3099 - acc: 0.9483Epoch 00020: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.3110 - acc: 0.9475 - val_loss: 0.4295 - val_acc: 0.9149
Epoch 22/200
190/191 [============================>.] - ETA: 0s - loss: 0.3187 - acc: 0.9462Epoch 00021: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.3181 - acc: 0.9465 - val_loss: 0.6009 - val_acc: 0.8693
Epoch 23/200
190/191 [============================>.] - ETA: 0s - loss: 0.2990 - acc: 0.9542Epoch 00022: val_acc improved from 0.93201 to 0.93663, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 62s - loss: 0.2984 - acc: 0.9544 - val_loss: 0.3617 - val_acc: 0.9366
Epoch 24/200
190/191 [============================>.] - ETA: 0s - loss: 0.2876 - acc: 0.9568Epoch 00023: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.2870 - acc: 0.9571 - val_loss: 0.4033 - val_acc: 0.9017
Epoch 25/200
190/191 [============================>.] - ETA: 0s - loss: 0.2818 - acc: 0.9572Epoch 00024: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.2813 - acc: 0.9574 - val_loss: 0.3932 - val_acc: 0.9195
Epoch 26/200
190/191 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9646Epoch 00025: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.2734 - acc: 0.9648 - val_loss: 0.3586 - val_acc: 0.9353
Epoch 27/200
190/191 [============================>.] - ETA: 0s - loss: 0.2747 - acc: 0.9591Epoch 00026: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.2758 - acc: 0.9588 - val_loss: 0.3593 - val_acc: 0.9294
Epoch 28/200
190/191 [============================>.] - ETA: 0s - loss: 0.2752 - acc: 0.9603Epoch 00027: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.2748 - acc: 0.9605 - val_loss: 0.4614 - val_acc: 0.8950
Epoch 29/200
190/191 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.9618Epoch 00028: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.2674 - acc: 0.9615 - val_loss: 0.3653 - val_acc: 0.9221
Epoch 30/200
190/191 [============================>.] - ETA: 0s - loss: 0.2624 - acc: 0.9648Epoch 00029: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.2620 - acc: 0.9650 - val_loss: 0.6205 - val_acc: 0.8739
Epoch 31/200
190/191 [============================>.] - ETA: 0s - loss: 0.2632 - acc: 0.9630Epoch 00030: val_acc improved from 0.93663 to 0.95050, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 61s - loss: 0.2631 - acc: 0.9632 - val_loss: 0.3214 - val_acc: 0.9505
Epoch 32/200
190/191 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9686Epoch 00031: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.2534 - acc: 0.9682 - val_loss: 0.4561 - val_acc: 0.8997
Epoch 33/200
190/191 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9674Epoch 00032: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.2532 - acc: 0.9676 - val_loss: 1.4637 - val_acc: 0.7663
Epoch 34/200
190/191 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.9691Epoch 00033: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.2480 - acc: 0.9692 - val_loss: 0.4060 - val_acc: 0.9327
Epoch 35/200
190/191 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9709Epoch 00034: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.2398 - acc: 0.9710 - val_loss: 0.4768 - val_acc: 0.9030
Epoch 36/200
190/191 [============================>.] - ETA: 0s - loss: 0.2381 - acc: 0.9722Epoch 00035: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.2386 - acc: 0.9718 - val_loss: 1.2999 - val_acc: 0.7545
Epoch 37/200
190/191 [============================>.] - ETA: 0s - loss: 0.2374 - acc: 0.9734Epoch 00036: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.2370 - acc: 0.9735 - val_loss: 0.5737 - val_acc: 0.8924
Epoch 38/200
190/191 [============================>.] - ETA: 0s - loss: 0.2335 - acc: 0.9744Epoch 00037: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.2348 - acc: 0.9740 - val_loss: 0.3813 - val_acc: 0.9406
Epoch 39/200
190/191 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9713Epoch 00038: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.2353 - acc: 0.9714 - val_loss: 0.4498 - val_acc: 0.9162
Epoch 40/200
190/191 [============================>.] - ETA: 0s - loss: 0.2255 - acc: 0.9741Epoch 00039: val_acc did not improve

Epoch 00039: reducing learning rate to 0.004999999888241291.
191/191 [==============================] - 61s - loss: 0.2252 - acc: 0.9742 - val_loss: 0.3147 - val_acc: 0.9419
Epoch 41/200
190/191 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9824Epoch 00040: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.2040 - acc: 0.9825 - val_loss: 0.4278 - val_acc: 0.9195
Epoch 42/200
190/191 [============================>.] - ETA: 0s - loss: 0.1939 - acc: 0.9861Epoch 00041: val_acc improved from 0.95050 to 0.96700, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 63s - loss: 0.1937 - acc: 0.9862 - val_loss: 0.2577 - val_acc: 0.9670
Epoch 43/200
190/191 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9847Epoch 00042: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1994 - acc: 0.9843 - val_loss: 0.3037 - val_acc: 0.9617
Epoch 44/200
190/191 [============================>.] - ETA: 0s - loss: 0.1930 - acc: 0.9876Epoch 00043: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1928 - acc: 0.9876 - val_loss: 0.2918 - val_acc: 0.9591
Epoch 45/200
190/191 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9874Epoch 00044: val_acc improved from 0.96700 to 0.97162, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 62s - loss: 0.1898 - acc: 0.9875 - val_loss: 0.2462 - val_acc: 0.9716
Epoch 46/200
190/191 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9896Epoch 00045: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1847 - acc: 0.9897 - val_loss: 0.3061 - val_acc: 0.9571
Epoch 47/200
190/191 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9863Epoch 00046: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.1889 - acc: 0.9863 - val_loss: 0.2614 - val_acc: 0.9630
Epoch 48/200
190/191 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9874Epoch 00047: val_acc improved from 0.97162 to 0.97228, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 62s - loss: 0.1890 - acc: 0.9875 - val_loss: 0.2527 - val_acc: 0.9723
Epoch 49/200
190/191 [============================>.] - ETA: 0s - loss: 0.1852 - acc: 0.9882Epoch 00048: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1851 - acc: 0.9883 - val_loss: 0.3588 - val_acc: 0.9353
Epoch 50/200
190/191 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9895Epoch 00049: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.1833 - acc: 0.9895 - val_loss: 0.3306 - val_acc: 0.9439
Epoch 51/200
190/191 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9902Epoch 00050: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1809 - acc: 0.9903 - val_loss: 0.2703 - val_acc: 0.9597
Epoch 52/200
190/191 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9884Epoch 00051: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1829 - acc: 0.9885 - val_loss: 0.2625 - val_acc: 0.9703
Epoch 53/200
190/191 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9916Epoch 00052: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1761 - acc: 0.9917 - val_loss: 0.4964 - val_acc: 0.9162
Epoch 54/200
190/191 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9913Epoch 00053: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1804 - acc: 0.9903 - val_loss: 0.2994 - val_acc: 0.9518
Epoch 55/200
190/191 [============================>.] - ETA: 0s - loss: 0.1967 - acc: 0.9846Epoch 00054: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.1965 - acc: 0.9847 - val_loss: 0.3474 - val_acc: 0.9459
Epoch 56/200
190/191 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9893Epoch 00055: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1789 - acc: 0.9894 - val_loss: 0.3340 - val_acc: 0.9538
Epoch 57/200
190/191 [============================>.] - ETA: 0s - loss: 0.1735 - acc: 0.9917Epoch 00056: val_acc did not improve

Epoch 00056: reducing learning rate to 0.0024999999441206455.
191/191 [==============================] - 61s - loss: 0.1734 - acc: 0.9917 - val_loss: 0.2816 - val_acc: 0.9624
Epoch 58/200
190/191 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9938Epoch 00057: val_acc improved from 0.97228 to 0.97426, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 62s - loss: 0.1678 - acc: 0.9939 - val_loss: 0.2433 - val_acc: 0.9743
Epoch 59/200
190/191 [============================>.] - ETA: 0s - loss: 0.1668 - acc: 0.9942Epoch 00058: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1667 - acc: 0.9942 - val_loss: 0.2298 - val_acc: 0.9710
Epoch 60/200
190/191 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9943Epoch 00059: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.1650 - acc: 0.9944 - val_loss: 0.3215 - val_acc: 0.9604
Epoch 61/200
190/191 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9952Epoch 00060: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.1624 - acc: 0.9953 - val_loss: 0.2619 - val_acc: 0.9670
Epoch 62/200
190/191 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9959Epoch 00061: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.1610 - acc: 0.9959 - val_loss: 0.2777 - val_acc: 0.9644
Epoch 63/200
190/191 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9947Epoch 00062: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1638 - acc: 0.9942 - val_loss: 0.2843 - val_acc: 0.9624
Epoch 64/200
190/191 [============================>.] - ETA: 0s - loss: 0.1633 - acc: 0.9944Epoch 00063: val_acc improved from 0.97426 to 0.97558, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 62s - loss: 0.1635 - acc: 0.9944 - val_loss: 0.2379 - val_acc: 0.9756
Epoch 65/200
190/191 [============================>.] - ETA: 0s - loss: 0.1619 - acc: 0.9947Epoch 00064: val_acc improved from 0.97558 to 0.97624, saving model to 2017-05-31_vgg.h5
191/191 [==============================] - 62s - loss: 0.1618 - acc: 0.9947 - val_loss: 0.2592 - val_acc: 0.9762
Epoch 66/200
190/191 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9955Epoch 00065: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1597 - acc: 0.9955 - val_loss: 0.3146 - val_acc: 0.9591
Epoch 67/200
190/191 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9948Epoch 00066: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1624 - acc: 0.9938 - val_loss: 0.2452 - val_acc: 0.9650
Epoch 68/200
190/191 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9935Epoch 00067: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1645 - acc: 0.9935 - val_loss: 0.3064 - val_acc: 0.9624
Epoch 69/200
190/191 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.9961Epoch 00068: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1580 - acc: 0.9962 - val_loss: 0.2508 - val_acc: 0.9736
Epoch 70/200
190/191 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9963Epoch 00069: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.1595 - acc: 0.9958 - val_loss: 0.2647 - val_acc: 0.9703
Epoch 71/200
190/191 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9957Epoch 00070: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1588 - acc: 0.9957 - val_loss: 0.2539 - val_acc: 0.9710
Epoch 72/200
190/191 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9956Epoch 00071: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1577 - acc: 0.9957 - val_loss: 0.2542 - val_acc: 0.9663
Epoch 73/200
190/191 [============================>.] - ETA: 0s - loss: 0.1593 - acc: 0.9952Epoch 00072: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1592 - acc: 0.9953 - val_loss: 0.2739 - val_acc: 0.9657
Epoch 74/200
190/191 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9956Epoch 00073: val_acc did not improve

Epoch 00073: reducing learning rate to 0.0012499999720603228.
191/191 [==============================] - 61s - loss: 0.1571 - acc: 0.9957 - val_loss: 0.2640 - val_acc: 0.9683
Epoch 75/200
190/191 [============================>.] - ETA: 0s - loss: 0.1559 - acc: 0.9966Epoch 00074: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1558 - acc: 0.9966 - val_loss: 0.2579 - val_acc: 0.9710
Epoch 76/200
190/191 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9970Epoch 00075: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1548 - acc: 0.9971 - val_loss: 0.2493 - val_acc: 0.9729
Epoch 77/200
190/191 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9978Epoch 00076: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1519 - acc: 0.9978 - val_loss: 0.2655 - val_acc: 0.9710
Epoch 78/200
190/191 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9978Epoch 00077: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1529 - acc: 0.9973 - val_loss: 0.2534 - val_acc: 0.9690
Epoch 79/200
190/191 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9974Epoch 00078: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1532 - acc: 0.9974 - val_loss: 0.2357 - val_acc: 0.9716
Epoch 80/200
190/191 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9977Epoch 00079: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.1520 - acc: 0.9977 - val_loss: 0.2519 - val_acc: 0.9729
Epoch 81/200
190/191 [============================>.] - ETA: 0s - loss: 0.1509 - acc: 0.9977Epoch 00080: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1509 - acc: 0.9977 - val_loss: 0.2577 - val_acc: 0.9703
Epoch 82/200
190/191 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9965Epoch 00081: val_acc did not improve

Epoch 00081: reducing learning rate to 0.0006249999860301614.
191/191 [==============================] - 62s - loss: 0.1540 - acc: 0.9965 - val_loss: 0.2506 - val_acc: 0.9696
Epoch 83/200
190/191 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.9970Epoch 00082: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1523 - acc: 0.9971 - val_loss: 0.2638 - val_acc: 0.9690
Epoch 84/200
190/191 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9979Epoch 00083: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1502 - acc: 0.9979 - val_loss: 0.2768 - val_acc: 0.9677
Epoch 85/200
190/191 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9973Epoch 00084: val_acc did not improve
191/191 [==============================] - 62s - loss: 0.1514 - acc: 0.9973 - val_loss: 0.2447 - val_acc: 0.9749
Epoch 86/200
190/191 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9979Epoch 00085: val_acc did not improve
191/191 [==============================] - 61s - loss: 0.1508 - acc: 0.9980 - val_loss: 0.2893 - val_acc: 0.9677
Epoch 00085: early stopping
Loading best model from check-point and testing...
                 precision    recall  f1-score   support

   Quarter-Rest       1.00      1.00      1.00        40
    Eighth-Note       1.00      0.97      0.99        39
Thirty-Two-Rest       0.93      0.97      0.95        40
          Sharp       1.00      0.95      0.97        40
       Cut-Time       1.00      0.95      0.97        40
       9-8-Time       1.00      0.97      0.99        40
       6-8-Time       1.00      1.00      1.00        40
      Half-Note       0.98      1.00      0.99        40
            Dot       0.97      0.97      0.97        40
     Whole-Note       0.98      1.00      0.99        40
   Quarter-Note       1.00      1.00      1.00        40
    Eighth-Rest       1.00      1.00      1.00        40
       3-8-Time       0.98      1.00      0.99        40
 Sixteenth-Note       0.93      1.00      0.96        40
         G-Clef       0.94      0.96      0.95        80
Thirty-Two-Note       0.98      1.00      0.99        40
       2-2-Time       1.00      1.00      1.00        40
      12-8-Time       1.00      0.95      0.97        39
        Natural       1.00      1.00      1.00        40
    Common-Time       0.99      0.97      0.98        79
       3-4-Time       0.97      0.97      0.97        40
Whole-Half-Rest       0.96      1.00      0.98        80
         F-Clef       0.90      0.88      0.89        40
       4-4-Time       1.00      0.97      0.99        40
Sixty-Four-Note       0.90      0.95      0.93        80
           Flat       0.95      0.90      0.92        40
       2-4-Time       0.97      0.91      0.94        79
        Barline       1.00      0.85      0.92        40
   Double-Sharp       0.91      0.91      0.91        79
         C-Clef       0.85      0.97      0.91        40
 Sixteenth-Rest       1.00      0.97      0.99        40
Sixty-Four-Rest       0.98      1.00      0.99        40

    avg / total       0.97      0.97      0.97      1515

Total Loss: 0.27196
Total Accuracy: 96.56766%
Total Error: 3.43234%
Execution time: 5359.4s
