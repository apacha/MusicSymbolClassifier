C:\Programming\Anaconda3-4.2.0\python.exe C:/Users/Alex/Repositories/MusicSymbolClassifier/HomusTrainer/TrainModel.py --delete_and_recreate_dataset_directory False --model_name vgg4
Using TensorFlow backend.
Training on dataset...
Found 48650 images belonging to 32 classes.
Found 6075 images belonging to 32 classes.
Found 6075 images belonging to 32 classes.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 224, 128, 32)      896       
_________________________________________________________________
batch_normalization_1 (Batch (None, 224, 128, 32)      128       
_________________________________________________________________
activation_1 (Activation)    (None, 224, 128, 32)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 224, 128, 32)      9248      
_________________________________________________________________
batch_normalization_2 (Batch (None, 224, 128, 32)      128       
_________________________________________________________________
activation_2 (Activation)    (None, 224, 128, 32)      0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 112, 64, 32)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 112, 64, 64)       18496     
_________________________________________________________________
batch_normalization_3 (Batch (None, 112, 64, 64)       256       
_________________________________________________________________
activation_3 (Activation)    (None, 112, 64, 64)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 112, 64, 64)       36928     
_________________________________________________________________
batch_normalization_4 (Batch (None, 112, 64, 64)       256       
_________________________________________________________________
activation_4 (Activation)    (None, 112, 64, 64)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 56, 32, 64)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 56, 32, 128)       73856     
_________________________________________________________________
batch_normalization_5 (Batch (None, 56, 32, 128)       512       
_________________________________________________________________
activation_5 (Activation)    (None, 56, 32, 128)       0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 56, 32, 128)       147584    
_________________________________________________________________
batch_normalization_6 (Batch (None, 56, 32, 128)       512       
_________________________________________________________________
activation_6 (Activation)    (None, 56, 32, 128)       0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 56, 32, 128)       147584    
_________________________________________________________________
batch_normalization_7 (Batch (None, 56, 32, 128)       512       
_________________________________________________________________
activation_7 (Activation)    (None, 56, 32, 128)       0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 28, 16, 128)       0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 28, 16, 256)       295168    
_________________________________________________________________
batch_normalization_8 (Batch (None, 28, 16, 256)       1024      
_________________________________________________________________
activation_8 (Activation)    (None, 28, 16, 256)       0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 28, 16, 256)       590080    
_________________________________________________________________
batch_normalization_9 (Batch (None, 28, 16, 256)       1024      
_________________________________________________________________
activation_9 (Activation)    (None, 28, 16, 256)       0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 28, 16, 256)       590080    
_________________________________________________________________
batch_normalization_10 (Batc (None, 28, 16, 256)       1024      
_________________________________________________________________
activation_10 (Activation)   (None, 28, 16, 256)       0         
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 14, 8, 256)        0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 14, 8, 512)        1180160   
_________________________________________________________________
batch_normalization_11 (Batc (None, 14, 8, 512)        2048      
_________________________________________________________________
activation_11 (Activation)   (None, 14, 8, 512)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 14, 8, 512)        2359808   
_________________________________________________________________
batch_normalization_12 (Batc (None, 14, 8, 512)        2048      
_________________________________________________________________
activation_12 (Activation)   (None, 14, 8, 512)        0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 14, 8, 512)        2359808   
_________________________________________________________________
batch_normalization_13 (Batc (None, 14, 8, 512)        2048      
_________________________________________________________________
activation_13 (Activation)   (None, 14, 8, 512)        0         
_________________________________________________________________
average_pooling2d_1 (Average (None, 7, 4, 512)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 14336)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                458784    
_________________________________________________________________
output_node (Activation)     (None, 32)                0         
=================================================================
Total params: 8,280,000
Trainable params: 8,274,240
Non-trainable params: 5,760
_________________________________________________________________
Model vgg4 loaded.
2017-06-02 15:04:21.000524: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 15:04:21.000766: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 15:04:21.000998: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 15:04:21.001583: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 15:04:21.001761: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 15:04:21.001932: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 15:04:21.002421: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 15:04:21.002582: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 15:04:21.324406: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:887] Found device 0 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:01:00.0
Total memory: 11.00GiB
Free memory: 9.12GiB
2017-06-02 15:04:21.324668: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:908] DMA: 0 
2017-06-02 15:04:21.324786: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:918] 0:   Y 
2017-06-02 15:04:21.324919: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)
Training for 200 epochs with initial learning rate of 0.01, weight-decay of 0.0001 and Nesterov Momentum of 0.9 ...
Additional parameters: Initialization: glorot_uniform, Minibatch-size: 64, Early stopping after 20 epochs without improvement
Data-Shape: (224, 128, 3), Reducing learning rate by factor to 0.5 respectively if not improved validation accuracy after 8 epochs
Data-augmentation: Zooming 20.0% randomly, rotating 10Â° randomly
Optimizer: Adadelta, with parameters {'rho': 0.95, 'lr': 1.0, 'decay': 0.0, 'epsilon': 1e-08}
Epoch 1/200
 10/761 [..............................] - ETA: 665s - loss: 6.2949 - acc: 0.04372017-06-02 15:04:37.979649: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:247] PoolAllocator: After 3729 get requests, put_count=3726 evicted_count=1000 eviction_rate=0.268384 and unsatisfied allocation rate=0.29579
2017-06-02 15:04:37.979925: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
760/761 [============================>.] - ETA: 0s - loss: 1.3218 - acc: 0.6877Epoch 00000: val_acc improved from -inf to 0.78831, saving model to 2017-06-02_vgg4.h5
761/761 [==============================] - 288s - loss: 1.3206 - acc: 0.6881 - val_loss: 0.9178 - val_acc: 0.7883
Epoch 2/200
760/761 [============================>.] - ETA: 0s - loss: 0.5470 - acc: 0.8903Epoch 00001: val_acc improved from 0.78831 to 0.91786, saving model to 2017-06-02_vgg4.h5
761/761 [==============================] - 279s - loss: 0.5467 - acc: 0.8904 - val_loss: 0.4711 - val_acc: 0.9179
Epoch 3/200
760/761 [============================>.] - ETA: 0s - loss: 0.4282 - acc: 0.9277Epoch 00002: val_acc did not improve
761/761 [==============================] - 277s - loss: 0.4280 - acc: 0.9277 - val_loss: 0.4744 - val_acc: 0.9167
Epoch 4/200
760/761 [============================>.] - ETA: 0s - loss: 0.3604 - acc: 0.9450Epoch 00003: val_acc improved from 0.91786 to 0.95078, saving model to 2017-06-02_vgg4.h5
761/761 [==============================] - 277s - loss: 0.3602 - acc: 0.9451 - val_loss: 0.3338 - val_acc: 0.9508
Epoch 5/200
760/761 [============================>.] - ETA: 0s - loss: 0.3088 - acc: 0.9564Epoch 00004: val_acc did not improve
761/761 [==============================] - 276s - loss: 0.3088 - acc: 0.9564 - val_loss: 0.3356 - val_acc: 0.9462
Epoch 6/200
760/761 [============================>.] - ETA: 0s - loss: 0.2769 - acc: 0.9623Epoch 00005: val_acc did not improve
761/761 [==============================] - 280s - loss: 0.2768 - acc: 0.9623 - val_loss: 0.5095 - val_acc: 0.8922
Epoch 7/200
760/761 [============================>.] - ETA: 0s - loss: 0.2532 - acc: 0.9670Epoch 00006: val_acc improved from 0.95078 to 0.95588, saving model to 2017-06-02_vgg4.h5
761/761 [==============================] - 286s - loss: 0.2530 - acc: 0.9670 - val_loss: 0.2779 - val_acc: 0.9559
Epoch 8/200
760/761 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9729Epoch 00007: val_acc did not improve
761/761 [==============================] - 284s - loss: 0.2290 - acc: 0.9730 - val_loss: 1.0878 - val_acc: 0.8000
Epoch 9/200
760/761 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9739Epoch 00008: val_acc did not improve
761/761 [==============================] - 284s - loss: 0.2127 - acc: 0.9738 - val_loss: 0.6753 - val_acc: 0.8856
Epoch 10/200
760/761 [============================>.] - ETA: 0s - loss: 0.1941 - acc: 0.9789Epoch 00009: val_acc improved from 0.95588 to 0.98337, saving model to 2017-06-02_vgg4.h5
761/761 [==============================] - 275s - loss: 0.1940 - acc: 0.9790 - val_loss: 0.1853 - val_acc: 0.9834
Epoch 11/200
760/761 [============================>.] - ETA: 0s - loss: 0.1852 - acc: 0.9796Epoch 00010: val_acc did not improve
761/761 [==============================] - 286s - loss: 0.1852 - acc: 0.9796 - val_loss: 0.1751 - val_acc: 0.9814
Epoch 12/200
760/761 [============================>.] - ETA: 0s - loss: 0.1745 - acc: 0.9817Epoch 00011: val_acc did not improve
761/761 [==============================] - 266s - loss: 0.1747 - acc: 0.9816 - val_loss: 1.3268 - val_acc: 0.7700
Epoch 13/200
760/761 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9825Epoch 00012: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.1679 - acc: 0.9825 - val_loss: 0.2045 - val_acc: 0.9654
Epoch 14/200
760/761 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9837Epoch 00013: val_acc did not improve
761/761 [==============================] - 263s - loss: 0.1607 - acc: 0.9837 - val_loss: 0.1593 - val_acc: 0.9826
Epoch 15/200
760/761 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9845Epoch 00014: val_acc did not improve
761/761 [==============================] - 263s - loss: 0.1519 - acc: 0.9844 - val_loss: 0.1869 - val_acc: 0.9732
Epoch 16/200
760/761 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9855Epoch 00015: val_acc improved from 0.98337 to 0.98617, saving model to 2017-06-02_vgg4.h5
761/761 [==============================] - 263s - loss: 0.1459 - acc: 0.9856 - val_loss: 0.1425 - val_acc: 0.9862
Epoch 17/200
760/761 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.9849Epoch 00016: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.1469 - acc: 0.9849 - val_loss: 0.1473 - val_acc: 0.9847
Epoch 18/200
760/761 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9863Epoch 00017: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.1389 - acc: 0.9864 - val_loss: 0.1784 - val_acc: 0.9745
Epoch 19/200
760/761 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9870Epoch 00018: val_acc did not improve
761/761 [==============================] - 263s - loss: 0.1365 - acc: 0.9870 - val_loss: 0.1520 - val_acc: 0.9824
Epoch 20/200
760/761 [============================>.] - ETA: 0s - loss: 0.1321 - acc: 0.9879Epoch 00019: val_acc did not improve
761/761 [==============================] - 263s - loss: 0.1321 - acc: 0.9878 - val_loss: 0.1761 - val_acc: 0.9727
Epoch 21/200
760/761 [============================>.] - ETA: 0s - loss: 0.1273 - acc: 0.9889Epoch 00020: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.1272 - acc: 0.9890 - val_loss: 0.1573 - val_acc: 0.9793
Epoch 22/200
760/761 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9890Epoch 00021: val_acc did not improve
761/761 [==============================] - 263s - loss: 0.1250 - acc: 0.9889 - val_loss: 0.4022 - val_acc: 0.9080
Epoch 23/200
760/761 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9889Epoch 00022: val_acc did not improve
761/761 [==============================] - 263s - loss: 0.1246 - acc: 0.9889 - val_loss: 0.1379 - val_acc: 0.9849
Epoch 24/200
760/761 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9887Epoch 00023: val_acc did not improve
761/761 [==============================] - 263s - loss: 0.1201 - acc: 0.9886 - val_loss: 0.2284 - val_acc: 0.9674
Epoch 25/200
760/761 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.9889Epoch 00024: val_acc did not improve

Epoch 00024: reducing learning rate to 0.5.
761/761 [==============================] - 263s - loss: 0.1202 - acc: 0.9889 - val_loss: 0.2209 - val_acc: 0.9602
Epoch 26/200
760/761 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9966Epoch 00025: val_acc improved from 0.98617 to 0.99802, saving model to 2017-06-02_vgg4.h5
761/761 [==============================] - 265s - loss: 0.0932 - acc: 0.9966 - val_loss: 0.0874 - val_acc: 0.9980
Epoch 27/200
760/761 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9970Epoch 00026: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0859 - acc: 0.9970 - val_loss: 0.0824 - val_acc: 0.9977
Epoch 28/200
760/761 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9976Epoch 00027: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0799 - acc: 0.9976 - val_loss: 0.0788 - val_acc: 0.9972
Epoch 29/200
760/761 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9976Epoch 00028: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0751 - acc: 0.9976 - val_loss: 0.0739 - val_acc: 0.9972
Epoch 30/200
760/761 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9977Epoch 00029: val_acc improved from 0.99802 to 0.99802, saving model to 2017-06-02_vgg4.h5
761/761 [==============================] - 265s - loss: 0.0708 - acc: 0.9977 - val_loss: 0.0688 - val_acc: 0.9980
Epoch 31/200
760/761 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9976Epoch 00030: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0680 - acc: 0.9976 - val_loss: 0.0657 - val_acc: 0.9979
Epoch 32/200
760/761 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9973Epoch 00031: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0658 - acc: 0.9973 - val_loss: 0.0730 - val_acc: 0.9949
Epoch 33/200
760/761 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9973Epoch 00032: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0636 - acc: 0.9973 - val_loss: 0.0672 - val_acc: 0.9957
Epoch 34/200
760/761 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9969Epoch 00033: val_acc improved from 0.99802 to 0.99835, saving model to 2017-06-02_vgg4.h5
761/761 [==============================] - 266s - loss: 0.0624 - acc: 0.9969 - val_loss: 0.0588 - val_acc: 0.9984
Epoch 35/200
760/761 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9971Epoch 00034: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0608 - acc: 0.9971 - val_loss: 0.0574 - val_acc: 0.9979
Epoch 36/200
760/761 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9971Epoch 00035: val_acc did not improve
761/761 [==============================] - 270s - loss: 0.0589 - acc: 0.9971 - val_loss: 0.0569 - val_acc: 0.9975
Epoch 37/200
760/761 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9969Epoch 00036: val_acc did not improve
761/761 [==============================] - 285s - loss: 0.0590 - acc: 0.9969 - val_loss: 0.0837 - val_acc: 0.9909
Epoch 38/200
760/761 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9971Epoch 00037: val_acc did not improve
761/761 [==============================] - 273s - loss: 0.0563 - acc: 0.9971 - val_loss: 0.0545 - val_acc: 0.9974
Epoch 39/200
760/761 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9969Epoch 00038: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0556 - acc: 0.9969 - val_loss: 0.0588 - val_acc: 0.9964
Epoch 40/200
760/761 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9965Epoch 00039: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0560 - acc: 0.9966 - val_loss: 0.0570 - val_acc: 0.9962
Epoch 41/200
760/761 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9968Epoch 00040: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0546 - acc: 0.9968 - val_loss: 0.0706 - val_acc: 0.9926
Epoch 42/200
760/761 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9964Epoch 00041: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0556 - acc: 0.9964 - val_loss: 0.0535 - val_acc: 0.9974
Epoch 43/200
760/761 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9963Epoch 00042: val_acc did not improve

Epoch 00042: reducing learning rate to 0.25.
761/761 [==============================] - 265s - loss: 0.0559 - acc: 0.9963 - val_loss: 0.0526 - val_acc: 0.9975
Epoch 44/200
760/761 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9988Epoch 00043: val_acc improved from 0.99835 to 0.99934, saving model to 2017-06-02_vgg4.h5
761/761 [==============================] - 265s - loss: 0.0468 - acc: 0.9988 - val_loss: 0.0476 - val_acc: 0.9993
Epoch 45/200
760/761 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9993Epoch 00044: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0441 - acc: 0.9993 - val_loss: 0.0428 - val_acc: 0.9990
Epoch 46/200
760/761 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9993Epoch 00045: val_acc improved from 0.99934 to 0.99951, saving model to 2017-06-02_vgg4.h5
761/761 [==============================] - 265s - loss: 0.0419 - acc: 0.9993 - val_loss: 0.0412 - val_acc: 0.9995
Epoch 47/200
760/761 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9994Epoch 00046: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0405 - acc: 0.9994 - val_loss: 0.0447 - val_acc: 0.9984
Epoch 48/200
760/761 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9993Epoch 00047: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0394 - acc: 0.9993 - val_loss: 0.0426 - val_acc: 0.9988
Epoch 49/200
760/761 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9994Epoch 00048: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0378 - acc: 0.9994 - val_loss: 0.0384 - val_acc: 0.9995
Epoch 50/200
760/761 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9993Epoch 00049: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0367 - acc: 0.9993 - val_loss: 0.0384 - val_acc: 0.9988
Epoch 51/200
760/761 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9993Epoch 00050: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0356 - acc: 0.9993 - val_loss: 0.0347 - val_acc: 0.9992
Epoch 52/200
760/761 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9993Epoch 00051: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0346 - acc: 0.9993 - val_loss: 0.0363 - val_acc: 0.9990
Epoch 53/200
760/761 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9992Epoch 00052: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0342 - acc: 0.9992 - val_loss: 0.0356 - val_acc: 0.9993
Epoch 54/200
760/761 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9995Epoch 00053: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0327 - acc: 0.9995 - val_loss: 0.0364 - val_acc: 0.9987
Epoch 55/200
760/761 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9992Epoch 00054: val_acc did not improve

Epoch 00054: reducing learning rate to 0.125.
761/761 [==============================] - 265s - loss: 0.0325 - acc: 0.9992 - val_loss: 0.0352 - val_acc: 0.9985
Epoch 56/200
760/761 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9996Epoch 00055: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0304 - acc: 0.9996 - val_loss: 0.0318 - val_acc: 0.9995
Epoch 57/200
760/761 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9998Epoch 00056: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0292 - acc: 0.9998 - val_loss: 0.0308 - val_acc: 0.9993
Epoch 58/200
760/761 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9997Epoch 00057: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0289 - acc: 0.9997 - val_loss: 0.0304 - val_acc: 0.9995
Epoch 59/200
760/761 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9997Epoch 00058: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0281 - acc: 0.9997 - val_loss: 0.0297 - val_acc: 0.9993
Epoch 60/200
760/761 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9998Epoch 00059: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0274 - acc: 0.9998 - val_loss: 0.0314 - val_acc: 0.9995
Epoch 61/200
760/761 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9998Epoch 00060: val_acc improved from 0.99951 to 0.99967, saving model to 2017-06-02_vgg4.h5
761/761 [==============================] - 265s - loss: 0.0266 - acc: 0.9998 - val_loss: 0.0269 - val_acc: 0.9997
Epoch 62/200
760/761 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9998Epoch 00061: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0259 - acc: 0.9998 - val_loss: 0.0298 - val_acc: 0.9993
Epoch 63/200
760/761 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9998Epoch 00062: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0257 - acc: 0.9998 - val_loss: 0.0266 - val_acc: 0.9995
Epoch 64/200
760/761 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9997Epoch 00063: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0251 - acc: 0.9997 - val_loss: 0.0284 - val_acc: 0.9995
Epoch 65/200
760/761 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9999Epoch 00064: val_acc improved from 0.99967 to 0.99984, saving model to 2017-06-02_vgg4.h5
761/761 [==============================] - 265s - loss: 0.0244 - acc: 0.9999 - val_loss: 0.0249 - val_acc: 0.9998
Epoch 66/200
760/761 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9999Epoch 00065: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0236 - acc: 0.9999 - val_loss: 0.0265 - val_acc: 0.9995
Epoch 67/200
760/761 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9998Epoch 00066: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0234 - acc: 0.9998 - val_loss: 0.0255 - val_acc: 0.9995
Epoch 68/200
760/761 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9998Epoch 00067: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0227 - acc: 0.9998 - val_loss: 0.0244 - val_acc: 0.9997
Epoch 69/200
760/761 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9998Epoch 00068: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0224 - acc: 0.9998 - val_loss: 0.0241 - val_acc: 0.9995
Epoch 70/200
760/761 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9998Epoch 00069: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0219 - acc: 0.9998 - val_loss: 0.0236 - val_acc: 0.9997
Epoch 71/200
760/761 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9999Epoch 00070: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0212 - acc: 0.9999 - val_loss: 0.0252 - val_acc: 0.9992
Epoch 72/200
760/761 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9997Epoch 00071: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0213 - acc: 0.9997 - val_loss: 0.0242 - val_acc: 0.9988
Epoch 73/200
760/761 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9996Epoch 00072: val_acc improved from 0.99984 to 1.00000, saving model to 2017-06-02_vgg4.h5
761/761 [==============================] - 266s - loss: 0.0211 - acc: 0.9996 - val_loss: 0.0202 - val_acc: 1.0000
Epoch 74/200
760/761 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 1.0000Epoch 00073: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0200 - acc: 1.0000 - val_loss: 0.0233 - val_acc: 0.9995
Epoch 75/200
760/761 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9999Epoch 00074: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0197 - acc: 0.9999 - val_loss: 0.0214 - val_acc: 0.9997
Epoch 76/200
760/761 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9999Epoch 00075: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0193 - acc: 0.9999 - val_loss: 0.0213 - val_acc: 0.9995
Epoch 77/200
760/761 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9998Epoch 00076: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0187 - acc: 0.9998 - val_loss: 0.0204 - val_acc: 0.9997
Epoch 78/200
760/761 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9997Epoch 00077: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0188 - acc: 0.9997 - val_loss: 0.0202 - val_acc: 0.9997
Epoch 79/200
760/761 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9998Epoch 00078: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0184 - acc: 0.9998 - val_loss: 0.0191 - val_acc: 0.9997
Epoch 80/200
760/761 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9998Epoch 00079: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0180 - acc: 0.9998 - val_loss: 0.0198 - val_acc: 0.9997
Epoch 81/200
760/761 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9999Epoch 00080: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0175 - acc: 0.9999 - val_loss: 0.0207 - val_acc: 0.9993
Epoch 82/200
760/761 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9998Epoch 00081: val_acc did not improve

Epoch 00081: reducing learning rate to 0.0625.
761/761 [==============================] - 265s - loss: 0.0172 - acc: 0.9998 - val_loss: 0.0204 - val_acc: 0.9995
Epoch 83/200
760/761 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9998Epoch 00082: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0170 - acc: 0.9998 - val_loss: 0.0185 - val_acc: 0.9997
Epoch 84/200
760/761 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9999Epoch 00083: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0165 - acc: 0.9999 - val_loss: 0.0186 - val_acc: 0.9997
Epoch 85/200
760/761 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9999Epoch 00084: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0164 - acc: 0.9999 - val_loss: 0.0183 - val_acc: 0.9997
Epoch 86/200
760/761 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9999Epoch 00085: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0162 - acc: 0.9999 - val_loss: 0.0184 - val_acc: 0.9997
Epoch 87/200
760/761 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9998Epoch 00086: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0161 - acc: 0.9998 - val_loss: 0.0182 - val_acc: 0.9997
Epoch 88/200
760/761 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9998Epoch 00087: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0158 - acc: 0.9998 - val_loss: 0.0198 - val_acc: 0.9993
Epoch 89/200
760/761 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9998Epoch 00088: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0158 - acc: 0.9998 - val_loss: 0.0166 - val_acc: 0.9998
Epoch 90/200
760/761 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9998Epoch 00089: val_acc did not improve

Epoch 00089: reducing learning rate to 0.03125.
761/761 [==============================] - 264s - loss: 0.0156 - acc: 0.9998 - val_loss: 0.0175 - val_acc: 0.9997
Epoch 91/200
760/761 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9999Epoch 00090: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0152 - acc: 0.9999 - val_loss: 0.0191 - val_acc: 0.9995
Epoch 92/200
760/761 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 1.0000Epoch 00091: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0150 - acc: 1.0000 - val_loss: 0.0162 - val_acc: 0.9998
Epoch 93/200
760/761 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9999Epoch 00092: val_acc did not improve
761/761 [==============================] - 264s - loss: 0.0150 - acc: 0.9999 - val_loss: 0.0183 - val_acc: 0.9995
Epoch 94/200
760/761 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9999Epoch 00093: val_acc did not improve
761/761 [==============================] - 265s - loss: 0.0151 - acc: 0.9999 - val_loss: 0.0172 - val_acc: 0.9997
Epoch 00093: early stopping
Loading best model from check-point and testing...
                 precision    recall  f1-score   support

       3-4-Time       1.00      1.00      1.00       160
   Quarter-Rest       1.00      1.00      1.00       158
 Sixteenth-Rest       1.00      1.00      1.00       160
          Sharp       1.00      1.00      1.00       160
    Eighth-Rest       1.00      1.00      1.00       160
Thirty-Two-Rest       1.00      1.00      1.00       160
Sixty-Four-Note       1.00      1.00      1.00       160
     Whole-Note       1.00      1.00      1.00       160
            Dot       1.00      1.00      1.00       160
       2-4-Time       1.00      1.00      1.00       160
       4-4-Time       1.00      1.00      1.00       160
 Sixteenth-Note       1.00      1.00      1.00       161
         C-Clef       0.99      1.00      1.00       160
         F-Clef       1.00      1.00      1.00       160
        Natural       1.00      1.00      1.00       320
       9-8-Time       1.00      1.00      1.00       160
       3-8-Time       1.00      1.00      1.00       160
       2-2-Time       1.00      1.00      1.00       159
Whole-Half-Rest       1.00      1.00      1.00       160
       6-8-Time       1.00      1.00      1.00       319
       Cut-Time       1.00      1.00      1.00       160
      Half-Note       1.00      1.00      1.00       320
         G-Clef       1.00      1.00      1.00       160
   Quarter-Note       1.00      1.00      1.00       160
      12-8-Time       0.99      1.00      1.00       320
   Double-Sharp       1.00      1.00      1.00       160
    Common-Time       1.00      1.00      1.00       319
Thirty-Two-Note       1.00      1.00      1.00       160
           Flat       1.00      0.99      1.00       319
        Barline       1.00      1.00      1.00       160
Sixty-Four-Rest       1.00      0.99      1.00       160
    Eighth-Note       1.00      1.00      1.00       160

    avg / total       1.00      1.00      1.00      6075

Total Loss: 0.02187
Total Accuracy: 99.95062%
Total Error: 0.04938%
Execution time: 25172.0s
